{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jA2Xy1xiPXe"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/037_charRNN/charRNN.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weJpWrJMiPXg"
   },
   "source": [
    "# Generación de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7pRNVZqiPXg"
   },
   "source": [
    "En este post vamos a entrenar una `red neuronal recurrente` para generar texto, carácter a carácter, inspirado en [CharRNN](https://github.com/karpathy/char-rnn). Nuestra red neuronal recibirá como entrada una secuencia de letras y deberá dar como salida la siguiente letra (la cual añadiremos a las entradas para volver a generar un nuevo carácter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXEmM7CziPXh"
   },
   "source": [
    "## Los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGzYphrHiPXi"
   },
   "source": [
    "Lo primero que necesitamos para lograr nuestro objetivo es un conjunto de datos. En este caso, al querer generar texto, nos servirá con un archivo con mucho texto que queramos imitar. Para ello descargaremos *Don Quijote de la Mancha*, la obra principal del escritor Miguel de Cervantes y una de las más relevantes en la literatura castellana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:39.405968Z",
     "start_time": "2020-08-31T14:37:38.446954Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "executionInfo": {
     "elapsed": 7284,
     "status": "ok",
     "timestamp": 1737039652018,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "6b53NVw4iPXi",
    "outputId": "9eb0ca18-df94-4cae-b1de-1b12f740f2be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=ccb24721b0b72d8d31a0a041205496f83794d882e6fc12172d392ad18b798217\n",
      "  Stored in directory: c:\\users\\ivan\\appdata\\local\\pip\\cache\\wheels\\40\\b3\\0f\\a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'el_quijote.txt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wget\n",
    "import wget\n",
    "\n",
    "wget.download('https://mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud/el_quijote.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:39.437968Z",
     "start_time": "2020-08-31T14:37:39.406971Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1737039764679,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "ND8vvarhiPXj",
    "outputId": "7e3bc96c-c2ea-47cb-e5b6-8d405a65f09c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DON QUIJOTE DE LA MANCHA\\nMiguel de Cervantes Saavedra\\n\\nPRIMERA PARTE\\nCAPÍTULO 1: Que trata de la condición y ejercicio del famoso hidalgo D. Quijote de la Mancha\\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, ada',\n",
       " 1038397)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"el_quijote.txt\", \"r\", encoding='utf-8')\n",
    "text = f.read()\n",
    "text[:300], len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO9FtIj7iPXj"
   },
   "source": [
    "Tenemos alrededor de 1 millón de carácteres en nuestro dataset, suficientes para generar texto de manera convincente como si fuésemos el manco de Lepanto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_4RpafZiPXk"
   },
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6AaWVANiPXk"
   },
   "source": [
    "Para poder darle este texto a nuestra red neuronal necesitamos transformarlo en números con los que podemos llevar a cabo las operaciones que tienen lugar en la red. Este proceso se conoce como `tokenización`. Existen muchas formas de llevar a cabo este proceso, en este caso simplemente sustituiremos cada carácter en nuestro texto por su posición en el siguiente vector de carácteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:39.453968Z",
     "start_time": "2020-08-31T14:37:39.440969Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1737039882719,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "9L0s3DZPiPXl",
    "outputId": "fa4285ec-958d-4c45-a001-c4d916116c4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0cñÑáÁéÉíÍóÓúÚ¿¡'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "all_characters = string.printable + \"ñÑáÁéÉíÍóÓúÚ¿¡\"\n",
    "all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:39.469968Z",
     "start_time": "2020-08-31T14:37:39.454970Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1737040054893,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "S0pcGoOOiPXq",
    "outputId": "ef9380bd-5355-45be-f473-f9a426fd6579"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "class Tokenizer():\n",
    "\n",
    "  def __init__(self):\n",
    "    self.all_characters = all_characters\n",
    "    self.n_characters = len(self.all_characters)\n",
    "\n",
    "  def text_to_seq(self, string):\n",
    "    seq = []\n",
    "    for c in range(len(string)):\n",
    "        try:\n",
    "            seq.append(self.all_characters.index(string[c]))\n",
    "        except:\n",
    "            continue\n",
    "    return seq\n",
    "\n",
    "  def seq_to_text(self, seq):\n",
    "    text = ''\n",
    "    for c in range(len(seq)):\n",
    "        text += self.all_characters[seq[c]]\n",
    "    return text\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.n_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEmGT37giPXr"
   },
   "source": [
    "El tokenizer puede convertir una secuencia de texto en números, y al revés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:39.485968Z",
     "start_time": "2020-08-31T14:37:39.470969Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1737040070577,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "RQKGc6jciPXr",
    "outputId": "466cdf37-f099-448e-86ec-e5d1f2f014b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28, 14, 100, 24, 27, 73, 94, 112, 26, 30, 104, 94, 29, 10, 21, 82]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.text_to_seq('señor, ¿qué tal?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:39.501968Z",
     "start_time": "2020-08-31T14:37:39.486970Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1737040086564,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "TIHuZ7m2iPXs",
    "outputId": "eca22c7f-b3da-4891-ed33-34773e4547f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'señor, ¿qué tal?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.seq_to_text([28, 14, 100, 24, 27, 73, 94, 112, 26, 30, 104, 94, 29, 10, 21, 82])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E67Qcw7iPXs"
   },
   "source": [
    "Ahora podemos tokenizar todo el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:39.756970Z",
     "start_time": "2020-08-31T14:37:39.503970Z"
    },
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1737040196813,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "sOKY7ei9iPXt"
   },
   "outputs": [],
   "source": [
    "text_encoded = tokenizer.text_to_seq(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEpZz9KhiPXt"
   },
   "source": [
    "> 💡 Pese a que podemos implementar nuestra lógica de tokenización para trabajar a nivel de carácteres, cuando trabajamos con palabras completas el proceso puede complicarse. Es por esto que existen muchas herramientas que ya implementan este tipo de procesado (y muchos otros) que podemos utilizar. Un ejemplo, especialmente integrado con `Pytorch`, es la librería [torchtext](https://pytorch.org/text/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJmKjOr-iPXt"
   },
   "source": [
    "## El *Dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpnrPijYiPXt"
   },
   "source": [
    "En primer lugar, vamos a separar nuestro texto en un conjunto de entrenamiento y otro de test. Cómo ya hemos hablado en posts anteriores, usaremos los datos de entrenamiento para entrenar nuestra red neuronal y los datos de test para calcular las métricas finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:39.772968Z",
     "start_time": "2020-08-31T14:37:39.757969Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1737040249998,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "0ntw53psiPXt",
    "outputId": "5929ef28-6c71-4111-8691-c8b3f24a4d66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(814065, 203517)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(text_encoded) * 80 // 100\n",
    "train = text_encoded[:train_size]\n",
    "test = text_encoded[train_size:]\n",
    "\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuuDO9a0iPXu"
   },
   "source": [
    "Para entrenar nuestra red, vamos a necesitar secuencias de texto de una longitud determinada. Podemos generar estas ventanas con la siguiente función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:42.399028Z",
     "start_time": "2020-08-31T14:37:39.773969Z"
    },
    "executionInfo": {
     "elapsed": 4557,
     "status": "ok",
     "timestamp": 1737040917330,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "v_NPRmTPiPXu"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def windows(text, window_size = 100):\n",
    "    start_index = 0\n",
    "    end_index = len(text) - window_size\n",
    "    text_windows = []\n",
    "    while start_index < end_index:\n",
    "      text_windows.append(text[start_index:start_index+window_size+1])\n",
    "      start_index += 1\n",
    "    return text_windows\n",
    "\n",
    "text_encoded_windows = windows(text_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFf8e6H8iPXu"
   },
   "source": [
    "Como puedes ver, hemos generado un número determinado de frases con la longitud especificada las cuales empiezan cada vez un carácter más a la derecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:42.415028Z",
     "start_time": "2020-08-31T14:37:42.400029Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1737040939283,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "hNswE3JeiPXv",
    "outputId": "af676f46-5610-45e0-e8d4-668c17362e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DON QUIJOTE DE LA MANCHA\n",
      "Miguel de Cervantes Saavedra\n",
      "\n",
      "PRIMERA PARTE\n",
      "CAPITULO 1: Que trata de la cond\n",
      "\n",
      "ON QUIJOTE DE LA MANCHA\n",
      "Miguel de Cervantes Saavedra\n",
      "\n",
      "PRIMERA PARTE\n",
      "CAPITULO 1: Que trata de la condi\n",
      "\n",
      "N QUIJOTE DE LA MANCHA\n",
      "Miguel de Cervantes Saavedra\n",
      "\n",
      "PRIMERA PARTE\n",
      "CAPITULO 1: Que trata de la condic\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.seq_to_text((text_encoded_windows[0])))\n",
    "print()\n",
    "print(tokenizer.seq_to_text((text_encoded_windows[1])))\n",
    "print()\n",
    "print(tokenizer.seq_to_text((text_encoded_windows[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thm0r8MiiPXv"
   },
   "source": [
    "Nuestro *dataset* de `Pytorch` se encargará de darnos cada una de estas frases, utilizando todos los carácteres excepto el último como entradas para la red y el último carácter como la etiqueta que usaremos durante el entrenamiento (la red deberá predecir la siguiente letra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1737041033522,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "UpyM62dHtqk2",
    "outputId": "782bce42-71e6-4ce2-be79-b4be3ab7e965"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1017482"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_encoded_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:43.505162Z",
     "start_time": "2020-08-31T14:37:42.416029Z"
    },
    "executionInfo": {
     "elapsed": 3623,
     "status": "ok",
     "timestamp": 1737041354891,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "StXgUySpiPXv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CharRNNDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, text_encoded_windows, train=True):\n",
    "    self.text = text_encoded_windows\n",
    "    self.train = train\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "\n",
    "  def __getitem__(self, ix):\n",
    "    if self.train:\n",
    "      return torch.tensor(self.text[ix][:-1]), torch.tensor(self.text[ix][-1])\n",
    "    return torch.tensor(self.text[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:46.441198Z",
     "start_time": "2020-08-31T14:37:43.506164Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5184,
     "status": "ok",
     "timestamp": 1737041392904,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "ZtH1qyUniPXv",
    "outputId": "ceeafede-092c-4d48-bee8-8e8b0d0b4714"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813965, 203417)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_encoded_windows = windows(train)\n",
    "test_text_encoded_windows = windows(test)\n",
    "\n",
    "dataset = {\n",
    "    'train': CharRNNDataset(train_text_encoded_windows),\n",
    "    'val': CharRNNDataset(test_text_encoded_windows)\n",
    "}\n",
    "\n",
    "dataloader = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=512, shuffle=True, pin_memory=True),\n",
    "    'val': torch.utils.data.DataLoader(dataset['val'], batch_size=2048, shuffle=False, pin_memory=True),\n",
    "}\n",
    "\n",
    "len(dataset['train']), len(dataset['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:46.472195Z",
     "start_time": "2020-08-31T14:37:46.443197Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1737041427621,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "-JOJpnkUiPXw",
    "outputId": "e5d72cf2-10e5-4c3b-e580-fe7903c07614"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DON QUIJOTE DE LA MANCHA\\nMiguel de Cervantes Saavedra\\n\\nPRIMERA PARTE\\nCAPITULO 1: Que trata de la con'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, output = dataset['train'][0]\n",
    "tokenizer.seq_to_text(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:46.488195Z",
     "start_time": "2020-08-31T14:37:46.473196Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1737041437480,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "uCvSFo_siPXw",
    "outputId": "8dc037c9-3b46-4070-b4fa-caa3536432d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.seq_to_text([output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oluPutsSiPXx"
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jcr61IFiPXx"
   },
   "source": [
    "Si bien hemos conseguido convertir nuestro texto a números, una red neuronal seguirá sin ser capaz de trabajar con nuestros datos ya que, como hemos visto en posts anteriores, éstos tienen que estar normalizados. Además, en función del `tokenizador` que utilicemos es posible que el  mismo carácter tenga asociados diferentes valores. Es por esto que necesitamos codificar nuestro texto de alguna manera.\n",
    "\n",
    "Una opción puede ser el `one-hot encoding`, al fin y al cabo podemos considerar cada letra como una categoría y que nuestra red nos de a la salida una distribución de probabilidad sobre todos los posibles carácteres. A continuación tienes un ejemplo de este tipo de codificación (utilizando palabras en vez de letras).\n",
    "\n",
    "![](https://i0.wp.com/shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com/wp-content/uploads/2018/01/one-hot-word-embedding-vectors.png?ssl=1)\n",
    "\n",
    "A nuestra red le daremos a la entrada un vector que representará cada elemento en el vocabulario. Este vector tendrá una longitud igual al número de elementos diferentes en el vocabulario, y estará lleno de ceros excepto por una posición (la posición que ocupe el elemento en concreto dentro del vocabulario, la lista de elementos únicos). En nuestro caso podríamos optar por esta alternativa, ya que apenas tenemos un centenar de carácteres diferentes. Sin embargo, cuando trabajemos con palabras, nuestros vocabularios serán enormes (¿cuántas palabras hay en el diccionario?). Esto implica que trabajar con una codificación `one-hot` será muy costoso (vectores muy grandes) e ineficiente (prácticamente llenos de ceros). Es por esto que utilizamos una mejor codificación: los `embeddings`\n",
    "\n",
    "![](https://i.stack.imgur.com/5gAnY.png)\n",
    "\n",
    "Un embedding es una matriz con un número de filas igual al tamaño del vocabulario y un número de columnas que nosotros decidiremos. Cada fila en la matriz representará la codificación de una palabara (o carácter en nuestro ejemplo). A diferencia de la codificación `one-hot`, estos vectores son densos (pueden tener valores diferentes de cero en cualquier posición). Además, estos valores son aprendidos por la red neuronal, de manera que podrá representar los datos de la mejor forma posible para llevar a cabo la tarea. En la figura anterior tienes un ejemplo de un embedding entrenado, ¿observas algún patrón?. Efectivamente, palabras similares tienen representaciones similares. Además, cada columna del embedding tiene un significado que permite establecer relaciones entre las diferentes representaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DtD87JBiPXx"
   },
   "source": [
    "> ⚡ ¿Qué resultado obtienes al restar el vector `boy` al vector `man` y sumarle el vector `girl`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVDwQiEgiPXx"
   },
   "source": [
    "En `Pytorch` tenemos esta capa implementada en la clase `torch.nn.Embedding`, y más adelante veremos como podemos utilizar `transfer learning` con embeddings pre-entrenados (lo cual nos dará una mejor representación de nuestro vocabulario desde el principio sin tener que entrenar esta capa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:46.504195Z",
     "start_time": "2020-08-31T14:37:46.489196Z"
    },
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1737042006010,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "J5ifXHiliPXx"
   },
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "  def __init__(self, input_size, embedding_size=128, hidden_size=256, num_layers=2, dropout=0.2):\n",
    "    super().__init__()\n",
    "    self.encoder = torch.nn.Embedding(input_size, embedding_size)\n",
    "    self.rnn = torch.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "    self.fc = torch.nn.Linear(hidden_size, input_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x, h = self.rnn(x)\n",
    "    y = self.fc(x[:,-1,:])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtChuV0niPXx"
   },
   "source": [
    "Nuestro modelo recibirá *batches* de frases con el índice de cada palabra que nos proporciona el `tokenizador`. A la salida tendremos una distribución de probabilidad sobre todos los posibles carácteres para cada frase del *batch*. Aquellos con mayor probabilidad serán los que la red cree que son buenos candidatos para seguir la frase recibida a la entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:46.647195Z",
     "start_time": "2020-08-31T14:37:46.505196Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1737042019492,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "IeTJkR8UiPXx",
    "outputId": "b7c598be-041e-4ca5-8c61-7eb7cca37449"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 114])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CharRNN(input_size=tokenizer.n_characters)\n",
    "outputs = model(torch.randint(0, tokenizer.n_characters, (64, 50)))\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uw9mXT7tiPXy"
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:37:46.709195Z",
     "start_time": "2020-08-31T14:37:46.649195Z"
    },
    "code_folding": [
     37
    ],
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737042043175,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "uG1JYUNUiPXy"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def fit(model, dataloader, epochs=10):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        bar = tqdm(dataloader['train'])\n",
    "        for batch in bar:\n",
    "            X, y = batch\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            bar.set_description(f\"loss {np.mean(train_loss):.5f}\")\n",
    "        bar = tqdm(dataloader['val'])\n",
    "        val_loss = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in bar:\n",
    "                X, y = batch\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_hat = model(X)\n",
    "                loss = criterion(y_hat, y)\n",
    "                val_loss.append(loss.item())\n",
    "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f}\")\n",
    "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f}\")\n",
    "\n",
    "def predict(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X = torch.tensor(X).to(device)\n",
    "        pred = model(X.unsqueeze(0))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando en: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verificar si CUDA está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Entrenando en: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T15:43:20.416599Z",
     "start_time": "2020-08-31T14:37:46.711195Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4322307,
     "status": "ok",
     "timestamp": 1737046374741,
     "user": {
      "displayName": "Manuel Llavador",
      "userId": "14521853358648384298"
     },
     "user_tz": -60
    },
    "id": "58WHxI89iPXy",
    "outputId": "1cac3df1-2b8d-4e6f-885a-6381391dc192"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1590 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CharRNN(input_size\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mn_characters)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 18\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, dataloader, epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_hat, y)\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Ivan\\anaconda3\\envs\\proyectoFlask\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ivan\\anaconda3\\envs\\proyectoFlask\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m, in \u001b[0;36mCharRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m      9\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m---> 10\u001b[0m   x, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m   y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:])\n\u001b[0;32m     12\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\Ivan\\anaconda3\\envs\\proyectoFlask\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ivan\\anaconda3\\envs\\proyectoFlask\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ivan\\anaconda3\\envs\\proyectoFlask\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1138\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1146\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CharRNN(input_size=tokenizer.n_characters)\n",
    "fit(model, dataloader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xs53MPjtiPX0"
   },
   "source": [
    "## Generando texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JcjUnwSiPX0"
   },
   "source": [
    "Una vez hemos entrenado nuestro modelo, podemos darle una frase para que genere la siguiente letra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T15:43:20.431602Z",
     "start_time": "2020-08-31T15:43:20.417599Z"
    },
    "id": "H8-w1clHiPX0",
    "outputId": "b9593bfd-cf91-4261-b127-4c6e882466c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = \"En un lugar de la mancha, \"\n",
    "X_new_encoded = tokenizer.text_to_seq(X_new)\n",
    "y_pred = predict(model, X_new_encoded)\n",
    "y_pred = torch.argmax(y_pred, axis=1)[0].item()\n",
    "tokenizer.seq_to_text([y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRWjWrKkiPX0"
   },
   "source": [
    "Podemos generar más letras añadiendo las predicciones como parte de la entrada, generando texto letra a letra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T15:43:20.653601Z",
     "start_time": "2020-08-31T15:43:20.433602Z"
    },
    "id": "tSkX1o1riPX0",
    "outputId": "d440f943-c0e0-4782-e31f-fdf260d8b44b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'En un lugar de la mancha, y el cura le dijo que el cura le habia de ser en la cabeza de la cabeza, y asi como lo habia de ser '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(100):\n",
    "  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])\n",
    "  y_pred = predict(model, X_new_encoded)\n",
    "  y_pred = torch.argmax(y_pred, axis=1)[0].item()\n",
    "  X_new += tokenizer.seq_to_text([y_pred])\n",
    "\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jugkeU0NiPX1"
   },
   "source": [
    "Cómo puedes ver el text generado puede ser repetitivo si simplemente nos quedamos con la letra con mayor probabilidad. Para generar texto con mayor variedad, es común elegir de manera aleatoria una letra de entre las que tienen mayor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T15:59:01.326415Z",
     "start_time": "2020-08-31T15:58:58.658570Z"
    },
    "id": "2W_Uu0oWiPX1",
    "outputId": "66476a5e-d0c2-46f4-eb34-1b309990888a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un lugar de la mancha, y el cura le dijo que el cura le habia de ser en la cabeza de la cabeza, y asi como lo habia de ser hacer eso dejado y coda esperaba su llamando su vida; y dijole que luego el quiere de mayor mal caballero, donde se la habia de comedimiento de su historia. Digo, senor licenciado otro dia ser bendicion de que su hiciera, a todo lo que digo, o aquellas casas alli la honrada, por mi marido, el fin que encierra; y asi, es Lotario a tierra de la galera de su milagro de su escudero jardin, dijo Cardenio:\n",
      "-Y de ese mi doncella se la preguntada -respondio Dorotea-, que es verdad que yo no se le trasladado, y con la menor lugar tan caballero, y con el lugar donde se entrete despidio; y aun el pudo la luz que entrara como consideraba en dejarle, de quien se muebale; porque a no te ha de quitar el arriero a quien dejaron en la si, que el habia de mas lanzon de un muerto, de consejo con Sancho que las resolas con contra el cura, aunque el aspero para estas segurosos que se pueda dar en vida, y a mi senora Amadis a mi siento, sino por los palabras de la tierra de caballeria que despierta de que los moros le habia dicho, no quisieron la vida, y tiraba en mi sidisimuleza, y el cura y a otra cosa me habia muerto de grandisima diligencia, a los dias en el camino, estando del vencimiento grande soledad y costumbre, pueda sierra al mal de la esfrace y minaguda, y dejando esto tiempo, y diciendo:\n",
      "-Digamos despues de ser, porque no les dijo que eran conde el suelo le dijo: porque la contradecia y se fuese esto, porque con muchos tiempos de alegro a agua, por poco ni le dejase; pero yo quedo la vuelta en su favor de su carta alguna, debe de cuya que se acontecio a ser punto en viendo con el almorio, y que tuviese mucha prevecion de sus hambres que les veeia en ellas, porque de mi queria, por cerrarme de mi suerte, si no lo sola! Por al cuerpo sin dudas los mios de las memorias, que tiene bien traidor del vizcaino, y luego, dijo:\n",
      "-¿Que me parece que soy como el cura se convera que esta a su paciencia que me halle decir, nos pudo conservo sus ojos que los arzobios habia puesto. Ambas no lo haya en el mundo y senora de los que le dijo que el se hallo en la punta de la caballeria y de la caballeria andante, y que es mas de la venta, y el cura decia el caballo de las manos de su casa y de la mano a la cabeza de su padre, y asi como lo haria en la carta de la cabeza, y asi, como a la mano a los caballeros andantes y entrambos, y que el de la vida que se le habia de ser en el mismo que el se escuchaba en su cargo de la cabeza de la mano a caballo, de particular de la mano de los demas de la caballeria, y asi como despues de ser en aquella mano a su padre y de aquella manera, y asi como lo conocio que el la habia de ser en la caballeria que aquella desengano estaba en la mano de los dias de la pena que el la conocieron, y tan a su senora Dulcinea, y que el se le pusiese de la vida, y la venta de su mano a la primera alma que le habia de dar a su escudero, porque si es mas de su amo que se le parecio que a ella se le hacia la cabeza de la cabeza, y la primera senora Dulcinea del Toboso de Dolenterosamente, yo, simple, honito nunca y le pusio las secretas; y si ellos soso!, puesto que os jayase la verdad vuestra merced a aquel diablo.\n",
      "Fiendo hayera vivia para el pensamiento y sin premiosa simplicion.\n",
      "Tu quedase con el mismo carto de un tambien dijo:\n",
      "-Hizo corta bien de tu reino, que no pudo cuatro deshonrado, ni mi amo sa aprovinar de camino, cuando en este sean una yerba\n",
      "como no salia como las tristas razones, y hacer medio los de Luscinda, que podia ver, y a la mano, sin servir de aquellas gente con muchas lamificas sea; y marques de puede hayamos.\n",
      "Cuando dices, senores, si sas porfia, y aun lo hase dicho conmigo y hombre desventura que ella pudo eso, ¿areno en alguna veces, o no apumo a soy, fortunaba; y ¿que tuvieron media hecho por esta gran impesimo retado.\n",
      "-Asi esto respondio el cabrero-, dijo Dorotea, que hablase la causa hija con sus sierras, que ellos le dejase de cualquieras ellos de las muchas gerraciones que yo bien, volvio a quien no estaban de\n"
     ]
    }
   ],
   "source": [
    "temp=1\n",
    "for i in range(1000):\n",
    "  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])\n",
    "  y_pred = predict(model, X_new_encoded)\n",
    "  y_pred = y_pred.view(-1).div(temp).exp()\n",
    "  top_i = torch.multinomial(y_pred, 1)[0]\n",
    "  predicted_char = tokenizer.all_characters[top_i]\n",
    "  X_new += predicted_char\n",
    "\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_Vwpaz5iPX1"
   },
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-ocb2fyiPX1"
   },
   "source": [
    "En este post hemos aprendido cómo implementar y entrenar una `red neuronal recurrente` para generar texto como si fuese Miguel de Cervantes. Para ello hemos utilizado su libro *Don Quijote de la Mancha* como dataset. En primer lugar, transformamos el texto en números gracias al proceso de la `tokenización`. Después, codificamos cada carácter en el dataset utilizando una capa `embedding`, que permitirá a la red neuronal encontrar la mejor representación posible de los datos para llevar a cabo su tarea. Para generar texto, le pedimos a la red que nos de una distribución de probabilidad sobre todos los posible carácteres a partir de una frase que le damos a la entrada. Utilizaremos esta distribución para seleccionar un carácter que siga con la frase de manera convincente. Podemos repetir este proceso para generar secuencias más largas."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "proyectoFlask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
